{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "XZywUuPAJUxT"
      },
      "outputs": [],
      "source": [
        "%%bash\n",
        "mkdir src\n",
        "mkdir report\n",
        "mkdir build\n",
        "rm -rf sample_data/"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "c:\\Users\\huyho\\AppData\\Local\\Programs\\Python\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Using device: cuda\n",
            "Loading LLM model: Qwen/Qwen3-1.7B\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Loading checkpoint shards: 100%|██████████| 2/2 [00:06<00:00,  3.35s/it]\n",
            "Some parameters are on the meta device because they were offloaded to the cpu.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Loading embedding model: sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\n",
            "KAG artifacts already exist. Skipping build phase.\n",
            "\n",
            "Solving Query 2: Sau bao lâu sinh viên có quyết định buộc thôi học, đơn vị đào tạo phải thông báo trả về địa phương nơi sinh viên có hộ khẩu thường trú?, trả lời bằng tiếng việt \n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "--- Step 1: Sub-question: Sau bao lâu sinh viên có quyết định buộc thôi học, đơn vị đào tạo phải thông báo trả về địa phương nơi sinh viên có hộ khẩu thường trú?, trả lời bằng tiếng việt  ---\n",
            "Sub-answer: <think>\n",
            "Okay, let's tackle this question. The user is asking how long after a student decides to withdraw (buộc thôi học) the educational institution must notify the student to return to the place of their permanent residence. The answer needs to be in Vietnamese.\n",
            "\n",
            "First, I check the retrieved chunks. There's a chunk from doc_vnu_chunk_5 that says \"1 tháng sau khi sinh viên có quyết định buộc thôi học, đơn vị đào tạo phải thông báo trả về địa phương nơi sinh viên có hộ khẩu thường trú.\" So that's a direct answer: 1 month after the student decides to withdraw, the institution must notify them to return to their permanent residence.\n",
            "\n",
            "Another chunk from doc_vnu_chunk_2 talks about the decision to withdraw and the criteria for it, but it doesn't mention the notification period. So the main answer comes from the first chunk. The user's history shows that the assistant previously answered this, so I need to confirm if the answer is correct based on the provided chunks.\n",
            "\n",
            "Since the chunk from doc_vnu_chunk_5 clearly states 1 month, and there's no conflicting information in the other chunks, the answer is straightforward. The user might be looking for confirmation that the answer is correct based on the given data. No missing information here, so the answer is valid.\n",
            "</think>\n",
            "\n",
            "Câu trả lời: Sau **1 tháng** kể từ khi sinh viên có quyết định buộc thôi học, đơn vị đào tạo phải thông báo trả về địa\n",
            "\n",
            "--- Synthesizing Final Answer ---\n",
            "\n",
            "Final Answer for Query 2:\n",
            "<think>\n",
            "Okay, let me process this. The user is asking about the notification period for a student's withdrawal and the return to their permanent residence. The provided answer from the chunk is 1 month. I need to make sure that this is the correct answer based on the given information. Since the chunk explicitly states 1 month, and there's no conflicting info elsewhere, the answer is straightforward. The user might be confirming that the answer is accurate. No need for additional steps here. Just present the answer as given.\n",
            "</think>\n",
            "\n",
            "Câu trả lời: Sau **1 tháng** kể từ khi sinh viên có quyết định buộc thôi học, đơn vị đào tạo phải thông báo trả về địa phương nơi sinh viên có hộ khẩu thường trú.\n"
          ]
        }
      ],
      "source": [
        "import os\n",
        "import torch\n",
        "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
        "from sentence_transformers import SentenceTransformer\n",
        "import faiss\n",
        "import networkx as nx\n",
        "import re\n",
        "from langchain.text_splitter import RecursiveCharacterTextSplitter\n",
        "from langchain.prompts import PromptTemplate\n",
        "from langchain.chains import LLMChain\n",
        "\n",
        "# --- Cấu hình ---\n",
        "# Kiểm tra xem CUDA có sẵn không và đặt thiết bị tương ứng\n",
        "DEVICE = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
        "print(f\"Using device: {DEVICE}\")\n",
        "\n",
        "# Model LLM (Sử dụng Qwen-1.4-7B-Chat, bạn có thể thay đổi)\n",
        "# Nếu bạn muốn dùng bản 4B, hãy thay bằng 'Qwen/Qwen-1_8B-Chat' hoặc tương tự\n",
        "# LLM_MODEL_NAME = \"Qwen/Qwen1.5-7B-Chat\"\n",
        "LLM_MODEL_NAME = \"Qwen/Qwen3-1.7B\" # Dùng bản nhỏ hơn để chạy nhanh hơn trên CPU/ít VRAM\n",
        "EMBEDDING_MODEL_NAME = \"sentence-transformers/paraphrase-multilingual-MiniLM-L12-v2\"\n",
        "\n",
        "# Đường dẫn lưu trữ\n",
        "FAISS_INDEX_PATH = \"my_faiss_index.index\"\n",
        "GRAPH_PATH = \"my_knowledge_graph.gml\"\n",
        "DOC_STORE_PATH = \"doc_store.json\" # Để lưu trữ text của chunk, map id với text\n",
        "\n",
        "# --- Khởi tạo Model ---\n",
        "print(f\"Loading LLM model: {LLM_MODEL_NAME}\")\n",
        "llm_tokenizer = AutoTokenizer.from_pretrained(LLM_MODEL_NAME, trust_remote_code=True)\n",
        "# Sử dụng device_map=\"auto\" nếu có nhiều GPU hoặc muốn Transformers tự quyết định\n",
        "# Đối với một GPU hoặc CPU, chỉ định rõ ràng là tốt nhất\n",
        "if DEVICE == \"cuda\":\n",
        "    llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_NAME,\n",
        "        torch_dtype=\"auto\", # Sử dụng bfloat16 nếu GPU hỗ trợ để tiết kiệm VRAM\n",
        "        device_map=\"auto\",  # Để transformers tự phân bổ lên GPU\n",
        "        trust_remote_code=True\n",
        "    )\n",
        "else: # CPU\n",
        "     llm_model = AutoModelForCausalLM.from_pretrained(\n",
        "        LLM_MODEL_NAME,\n",
        "        torch_dtype=torch.float32, # CPU thường dùng float32\n",
        "        trust_remote_code=True\n",
        "    ).to(DEVICE)\n",
        "\n",
        "print(f\"Loading embedding model: {EMBEDDING_MODEL_NAME}\")\n",
        "embedding_model = SentenceTransformer(EMBEDDING_MODEL_NAME, device=DEVICE)\n",
        "\n",
        "# --- Helper Functions ---\n",
        "def get_llm_response(prompt_text, max_new_tokens=250):\n",
        "    messages = [{\"role\": \"system\", \"content\": \"You are a helpful assistant.\"},\n",
        "                {\"role\": \"user\", \"content\": prompt_text}]\n",
        "    text = llm_tokenizer.apply_chat_template(messages, tokenize=False, add_generation_prompt=True)\n",
        "    model_inputs = llm_tokenizer([text], return_tensors=\"pt\").to(DEVICE)\n",
        "\n",
        "    generated_ids = llm_model.generate(\n",
        "        model_inputs.input_ids,\n",
        "        max_new_tokens=max_new_tokens,\n",
        "        # pad_token_id=llm_tokenizer.eos_token_id # Quan trọng với một số model\n",
        "    )\n",
        "    generated_ids = [\n",
        "        output_ids[len(input_ids):] for input_ids, output_ids in zip(model_inputs.input_ids, generated_ids)\n",
        "    ]\n",
        "    response = llm_tokenizer.batch_decode(generated_ids, skip_special_tokens=True)[0]\n",
        "    return response.strip()\n",
        "\n",
        "def get_embeddings(texts):\n",
        "    return embedding_model.encode(texts, convert_to_tensor=True, device=DEVICE)\n",
        "\n",
        "# --- KAG-Builder ---\n",
        "class KAGBuilder:\n",
        "    def __init__(self, chunk_size=500, chunk_overlap=50):\n",
        "        self.text_splitter = RecursiveCharacterTextSplitter(\n",
        "            chunk_size=chunk_size,\n",
        "            chunk_overlap=chunk_overlap\n",
        "        )\n",
        "        self.graph = nx.Graph()\n",
        "        self.doc_store = {} # id_chunk -> text_chunk\n",
        "\n",
        "    def _extract_entities_simple(self, text_chunk):\n",
        "        # Đơn giản hóa: trích xuất các từ viết hoa hoặc cụm từ đáng chú ý\n",
        "        # Trong thực tế, bạn sẽ dùng LLM cho việc này\n",
        "        prompt = f\"\"\"\n",
        "        Extract up to 3 key entities (people, organizations, locations, specific concepts) from the following text.\n",
        "        Return them as a comma-separated list. If no clear entities, return \"None\".\n",
        "\n",
        "        Text:\n",
        "        \"{text_chunk}\"\n",
        "\n",
        "        Entities:\n",
        "        \"\"\"\n",
        "        entities_str = get_llm_response(prompt, max_new_tokens=50)\n",
        "        if entities_str.lower() == \"none\":\n",
        "            return []\n",
        "        return [e.strip() for e in entities_str.split(',') if e.strip()]\n",
        "\n",
        "\n",
        "    def build_from_texts(self, texts_with_sources):\n",
        "        \"\"\"\n",
        "        texts_with_sources: list of tuples, e.g., [(\"content of doc1\", \"doc1_id\"), ...]\n",
        "        \"\"\"\n",
        "        all_chunks = []\n",
        "        chunk_id_counter = 0\n",
        "\n",
        "        for text_content, source_id in texts_with_sources:\n",
        "            chunks = self.text_splitter.split_text(text_content)\n",
        "            for i, chunk_text in enumerate(chunks):\n",
        "                chunk_id = f\"{source_id}_chunk_{i}\"\n",
        "                self.doc_store[chunk_id] = chunk_text\n",
        "                all_chunks.append({\"id\": chunk_id, \"text\": chunk_text, \"source\": source_id})\n",
        "\n",
        "                # Thêm chunk vào đồ thị\n",
        "                self.graph.add_node(chunk_id, type=\"chunk\", source=source_id, text=chunk_text[:100]+\"...\") # Lưu 1 phần text để debug\n",
        "\n",
        "                # Trích xuất và thêm thực thể (đơn giản)\n",
        "                entities = self._extract_entities_simple(chunk_text)\n",
        "                for entity_name in entities:\n",
        "                    # Chuẩn hóa tên thực thể (ví dụ: viết thường)\n",
        "                    normalized_entity = entity_name.lower().strip()\n",
        "                    if not self.graph.has_node(normalized_entity):\n",
        "                        self.graph.add_node(normalized_entity, type=\"entity\")\n",
        "                    self.graph.add_edge(chunk_id, normalized_entity, type=\"mentions\")\n",
        "                \n",
        "                chunk_id_counter += 1\n",
        "                if chunk_id_counter % 10 == 0:\n",
        "                    print(f\"Processed {chunk_id_counter} chunks...\")\n",
        "\n",
        "\n",
        "        # Tạo embeddings cho tất cả các chunk\n",
        "        chunk_texts_for_embedding = [chunk['text'] for chunk in all_chunks]\n",
        "        chunk_ids_for_embedding = [chunk['id'] for chunk in all_chunks]\n",
        "\n",
        "        if not chunk_texts_for_embedding:\n",
        "            print(\"No chunks to build index from.\")\n",
        "            return\n",
        "\n",
        "        print(\"Generating embeddings for chunks...\")\n",
        "        embeddings = get_embeddings(chunk_texts_for_embedding).cpu().numpy()\n",
        "\n",
        "        # Xây dựng FAISS index\n",
        "        dimension = embeddings.shape[1]\n",
        "        self.faiss_index = faiss.IndexFlatL2(dimension)\n",
        "        self.faiss_index = faiss.IndexIDMap(self.faiss_index) # Để map với ID của chunk\n",
        "\n",
        "        # Tạo một mảng các ID số cho FAISS\n",
        "        # Chúng ta sẽ lưu mapping từ id số này về id string của chunk\n",
        "        self.faiss_id_to_chunk_id = {i: chunk_id for i, chunk_id in enumerate(chunk_ids_for_embedding)}\n",
        "        numeric_ids = [i for i in range(len(chunk_ids_for_embedding))]\n",
        "        \n",
        "        self.faiss_index.add_with_ids(embeddings, numeric_ids)\n",
        "        print(f\"FAISS index built with {self.faiss_index.ntotal} vectors.\")\n",
        "\n",
        "        # Lưu trữ\n",
        "        faiss.write_index(self.faiss_index, FAISS_INDEX_PATH)\n",
        "        nx.write_gml(self.graph, GRAPH_PATH)\n",
        "        import json\n",
        "        with open(DOC_STORE_PATH, 'w') as f:\n",
        "            json.dump({\"doc_store\": self.doc_store, \"faiss_id_map\": self.faiss_id_to_chunk_id}, f)\n",
        "        print(\"Builder process completed and artifacts saved.\")\n",
        "\n",
        "\n",
        "# --- KAG-Solver ---\n",
        "class KAGSolver:\n",
        "    def __init__(self, top_k_retrieval=3):\n",
        "        self.faiss_index = faiss.read_index(FAISS_INDEX_PATH)\n",
        "        self.graph = nx.read_gml(GRAPH_PATH)\n",
        "        import json\n",
        "        with open(DOC_STORE_PATH, 'r') as f:\n",
        "            saved_data = json.load(f)\n",
        "            self.doc_store = saved_data['doc_store']\n",
        "            # faiss_id_map từ string (do JSON) sang int\n",
        "            self.faiss_id_to_chunk_id = {int(k): v for k, v in saved_data['faiss_id_map'].items()}\n",
        "\n",
        "        self.top_k = top_k_retrieval\n",
        "        self.reasoning_prompt_template = PromptTemplate(\n",
        "            input_variables=[\"original_query\", \"history\", \"current_sub_question\", \"retrieved_context\"],\n",
        "            template=\"\"\"\n",
        "            Bạn là một trợ lý AI giúp trả lời một câu hỏi phức tạp bằng cách chia nhỏ nó ra.\n",
        "            Câu hỏi gốc: {original_query}\n",
        "\n",
        "            Lịch sử suy luận (các câu hỏi phụ trước đó và câu trả lời của chúng):\n",
        "            {history}\n",
        "\n",
        "            Câu hỏi phụ hiện tại: {current_sub_question}\n",
        "\n",
        "            Ngữ cảnh được truy xuất cho câu hỏi phụ hiện tại:\n",
        "            ---\n",
        "            {retrieved_context}\n",
        "            ---\n",
        "\n",
        "            Dựa trên ngữ cảnh được truy xuất và lịch sử suy luận, hãy trả lời Câu hỏi phụ hiện tại.\n",
        "            Nếu ngữ cảnh không đủ, hãy nêu rõ điều đó và gợi ý những gì có thể còn thiếu.\n",
        "            Câu trả lời:\n",
        "            \"\"\"\n",
        "        )\n",
        "        self.synthesis_prompt_template = PromptTemplate(\n",
        "            input_variables=[\"original_query\", \"reasoning_trace\"],\n",
        "            template=\"\"\"\n",
        "            Dựa trên câu hỏi gốc sau và chuỗi suy luận từng bước,\n",
        "            hãy đưa ra một câu trả lời tổng hợp đầy đủ cho câu hỏi gốc.\n",
        "            Kết hợp thông tin một cách mạch lạc.\n",
        "\n",
        "            Câu hỏi gốc: {original_query}\n",
        "\n",
        "            Chuỗi suy luận:\n",
        "            {reasoning_trace}\n",
        "\n",
        "            Câu trả lời tổng hợp cuối cùng:\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "    def _retrieve_from_vector_db(self, query_text):\n",
        "        query_embedding = get_embeddings([query_text]).cpu().numpy()\n",
        "        distances, indices = self.faiss_index.search(query_embedding, self.top_k)\n",
        "        \n",
        "        retrieved_chunk_texts = []\n",
        "        for i in range(len(indices[0])):\n",
        "            faiss_numeric_id = indices[0][i]\n",
        "            if faiss_numeric_id != -1: # FAISS trả về -1 nếu không đủ k kết quả\n",
        "                chunk_id_str = self.faiss_id_to_chunk_id.get(faiss_numeric_id)\n",
        "                if chunk_id_str and chunk_id_str in self.doc_store:\n",
        "                    retrieved_chunk_texts.append(f\"Chunk ID: {chunk_id_str}\\nContent: {self.doc_store[chunk_id_str]}\\n---\")\n",
        "        return \"\\n\".join(retrieved_chunk_texts)\n",
        "\n",
        "    def _retrieve_from_kg(self, entities):\n",
        "        # Đơn giản: tìm các chunk liên quan đến thực thể\n",
        "        kg_info = []\n",
        "        for entity in entities:\n",
        "            normalized_entity = entity.lower().strip()\n",
        "            if self.graph.has_node(normalized_entity):\n",
        "                kg_info.append(f\"Knowledge Graph information for entity '{entity}':\")\n",
        "                for neighbor in self.graph.neighbors(normalized_entity):\n",
        "                    if self.graph.nodes[neighbor]['type'] == 'chunk':\n",
        "                        kg_info.append(f\"  - Mentioned in chunk: {neighbor} (Source: {self.graph.nodes[neighbor].get('source', 'N/A')})\")\n",
        "                        # Bạn có thể lấy thêm text của chunk này từ self.doc_store nếu cần\n",
        "        return \"\\n\".join(kg_info)\n",
        "\n",
        "    def _plan_steps(self, original_query):\n",
        "        # Sử dụng LLM để chia câu hỏi thành các bước\n",
        "        # Cho baseline, chúng ta có thể yêu cầu 3 bước\n",
        "        prompt = f\"\"\"\n",
        "        Phân tích câu hỏi phức tạp sau thành một chuỗi gồm 3 câu hỏi phụ đơn giản hơn.\n",
        "        Mỗi câu hỏi phụ nên dựa trên câu trước đó để giúp trả lời câu hỏi gốc.\n",
        "        Trả về các câu hỏi phụ dưới dạng danh sách được đánh số.\n",
        "\n",
        "        Câu hỏi gốc: \"{original_query}\"\n",
        "\n",
        "        Các câu hỏi phụ:\n",
        "        1. ...\n",
        "        2. ...\n",
        "        3. ...\n",
        "        \"\"\"\n",
        "        plan_str = get_llm_response(prompt, max_new_tokens=150)\n",
        "        # Phân tích plan_str để lấy các câu hỏi con\n",
        "        sub_questions = []\n",
        "        for line in plan_str.split('\\n'):\n",
        "            match = re.match(r\"^\\d+\\.\\s*(.+)\", line)\n",
        "            if match:\n",
        "                sub_questions.append(match.group(1).strip())\n",
        "        \n",
        "        # Nếu không phân tích được, trả về một câu hỏi mặc định\n",
        "        if not sub_questions:\n",
        "            return [original_query] # fallback\n",
        "        return sub_questions\n",
        "\n",
        "    def solve(self, original_query, max_steps=3):\n",
        "        sub_questions = self._plan_steps(original_query)\n",
        "        if not sub_questions:\n",
        "            print(\"Could not plan steps. Answering directly (basic RAG).\")\n",
        "            context = self._retrieve_from_vector_db(original_query)\n",
        "            # (Tùy chọn: trích xuất thực thể từ original_query và lấy thông tin KG)\n",
        "            final_prompt = f\"Original Query: {original_query}\\nContext:\\n{context}\\nAnswer:\"\n",
        "            return get_llm_response(final_prompt)\n",
        "\n",
        "        reasoning_history = []\n",
        "        reasoning_trace_for_synthesis = \"\"\n",
        "\n",
        "        for i, sub_q_text in enumerate(sub_questions):\n",
        "            if i >= max_steps:\n",
        "                break\n",
        "            \n",
        "            print(f\"\\n--- Step {i+1}: Sub-question: {sub_q_text} ---\")\n",
        "\n",
        "            # 1. Truy xuất từ Vector DB cho câu hỏi con hiện tại\n",
        "            retrieved_chunks = self._retrieve_from_vector_db(sub_q_text)\n",
        "            \n",
        "            # 2. (Tùy chọn) Trích xuất thực thể từ câu hỏi con và truy xuất KG\n",
        "            # (Đây là phần bạn có thể làm phức tạp hơn)\n",
        "            # entity_extraction_prompt = f\"Extract key entities from this question: \\\"{sub_q_text}\\\". Return as comma-separated list.\"\n",
        "            # entities_str = get_llm_response(entity_extraction_prompt, max_new_tokens=30)\n",
        "            # entities_in_sub_q = [e.strip() for e in entities_str.split(',') if e.strip()]\n",
        "            # kg_context = self._retrieve_from_kg(entities_in_sub_q)\n",
        "            # combined_context = f\"Vector DB Chunks:\\n{retrieved_chunks}\\n\\nKnowledge Graph Context:\\n{kg_context}\"\n",
        "            combined_context = f\"Retrieved Chunks:\\n{retrieved_chunks}\" # Giữ đơn giản\n",
        "\n",
        "            # 3. Tạo prompt và gọi LLM để trả lời câu hỏi con\n",
        "            current_history_str = \"\\n\".join([f\"  - Q: {item['q']}\\n    A: {item['a']}\" for item in reasoning_history])\n",
        "            \n",
        "            step_prompt_input = {\n",
        "                \"original_query\": original_query,\n",
        "                \"history\": current_history_str if current_history_str else \"No previous steps.\",\n",
        "                \"current_sub_question\": sub_q_text,\n",
        "                \"retrieved_context\": combined_context if combined_context else \"No context retrieved.\"\n",
        "            }\n",
        "            step_prompt = self.reasoning_prompt_template.format(**step_prompt_input)\n",
        "            # print(f\"DEBUG: Step Prompt:\\n{step_prompt}\")\n",
        "            \n",
        "            sub_answer = get_llm_response(step_prompt, max_new_tokens=300)\n",
        "            print(f\"Sub-answer: {sub_answer}\")\n",
        "\n",
        "            reasoning_history.append({\"q\": sub_q_text, \"a\": sub_answer})\n",
        "            reasoning_trace_for_synthesis += f\"Sub-Question {i+1}: {sub_q_text}\\nSub-Answer {i+1}: {sub_answer}\\n\\n\"\n",
        "\n",
        "        # 4. Tổng hợp câu trả lời cuối cùng\n",
        "        print(\"\\n--- Synthesizing Final Answer ---\")\n",
        "        synthesis_prompt_input = {\n",
        "            \"original_query\": original_query,\n",
        "            \"reasoning_trace\": reasoning_trace_for_synthesis\n",
        "        }\n",
        "        final_prompt = self.synthesis_prompt_template.format(**synthesis_prompt_input)\n",
        "        # print(f\"DEBUG: Synthesis Prompt:\\n{final_prompt}\")\n",
        "        final_answer = get_llm_response(final_prompt, max_new_tokens=500)\n",
        "        return final_answer\n",
        "\n",
        "# --- Main Execution ---\n",
        "if __name__ == \"__main__\":\n",
        "    # --- Giai đoạn xây dựng (chỉ chạy một lần hoặc khi dữ liệu thay đổi) ---\n",
        "    # Kiểm tra xem index đã tồn tại chưa để tránh xây dựng lại không cần thiết\n",
        "    if not os.path.exists(FAISS_INDEX_PATH) or not os.path.exists(GRAPH_PATH):\n",
        "        print(\"Building KAG artifacts...\")\n",
        "        builder = KAGBuilder(chunk_size=300, chunk_overlap=30) # Giảm chunk size để có nhiều chunk hơn\n",
        "        \n",
        "        # Ví dụ dữ liệu (thay thế bằng dữ liệu thực của bạn)\n",
        "        sample_docs = [\n",
        "            (\n",
        "                \"Điều 49. Xử lý học vụ Sau mỗi học kỳ chính, đơn vị đào tạo thực hiện xử lý học vụ. \"\n",
        "                \"Kết quả học tập của học kỳ phụ sẽ được tính vào kết quả học tập của học kỳ chính tiếp theo. \"\n",
        "                \"1. Cảnh báo học vụ  Đầu mỗi học kỳ, đơn vị đào tạo cảnh báo đối với những sinh viên có \"\n",
        "                \"điểm trung bình chung học kỳ đạt từ 0,80 đến dưới 0,85 đối với học kỳ đầu của khóa học; \"\n",
        "                \"đạt từ 1,00 đến dưới 1,10 đối với các học kỳ tiếp theo hoặc đạt từ 1,10 đến dưới 1,20 \"\n",
        "                \"đối với 2 học kỳ liên tiếp. 2. Thôi học Sinh viên được thôi học nếu có đơn xin thôi học \"\n",
        "                \"và được Thủ trưởng đơn vị đào tạo ra quyết định đồng ý. 3. Buộc thôi học Sau mỗi học kỳ, \"\n",
        "                \"sinh viên bị buộc thôi học nếu thuộc một trong các trường hợp sau: a) Có điểm trung bình \"\n",
        "                \"chung học kỳ đạt dưới 0,80 đối với học kỳ đầu của khóa học; đạt dưới 1,00 đối với các học kỳ \"\n",
        "                \"tiếp theo hoặc đạt dưới 1,10 đối với 2 học kỳ liên tiếp; b) Có điểm trung bình chung tích lũy \"\n",
        "                \"đạt dưới 1,20 đối với sinh viên năm thứ nhất; dưới 1,40 đối với sinh viên năm thứ hai; \"\n",
        "                \"dưới 1,60 đối với sinh viên năm thứ ba hoặc dưới 1,80 đối với sinh viên các năm tiếp theo \"\n",
        "                \"và cuối khóa; c) Vượt quá thời gian tối đa được phép học quy định tại khoản 2, Điều 24 \"\n",
        "                \"của Quy chế này; d) Bị kỷ luật lần thứ hai vì lý do thi hộ hoặc nhờ người thi hộ theo \"\n",
        "                \"quy định tại mục d, khoản 10, Điều 40 của Quy chế này hoặc bị kỷ luật ở mức xóa tên \"\n",
        "                \"khỏi danh sách sinh viên của trường.Chậm nhất 1 tháng sau khi sinh viên có quyết định \"\n",
        "                \"buộc thôi học, đơn vị đào tạo phải thông báo trả về địa phương nơi sinh viên có hộ khẩu thường trú\",\n",
        "                \"doc_vnu\"\n",
        "            )\n",
        "        ]\n",
        "        builder.build_from_texts(sample_docs)\n",
        "    else:\n",
        "        print(\"KAG artifacts already exist. Skipping build phase.\")\n",
        "\n",
        "    # --- Giai đoạn giải quyết truy vấn ---\n",
        "    solver = KAGSolver(top_k_retrieval=2) # Lấy ít chunk hơn cho mỗi bước\n",
        "    \n",
        "    # query1 = \"Who are the pioneers of AI and what are their main contributions, especially regarding NLP?\"\n",
        "    # print(f\"\\nSolving Query 1: {query1}\")\n",
        "    # answer1 = solver.solve(query1)\n",
        "    # print(f\"\\nFinal Answer for Query 1:\\n{answer1}\")\n",
        "\n",
        "    query2 = \"Cho tôi hỏi khi nào sinh viên được thôi học ? Khi nào sinh viên bị buộc thôi học? Sau bao lâu sinh viên có quyết định buộc thôi học, đơn vị đào tạo phải thông báo trả về địa phương nơi sinh viên có hộ khẩu thường trú?, trả lời bằng tiếng việt \"\n",
        "    print(f\"\\nSolving Query 2: {query2}\")\n",
        "    answer2 = solver.solve(query2, max_steps=3) # Giới hạn số bước cho truy vấn này\n",
        "    print(f\"\\nFinal Answer for Query 2:\\n{answer2}\")"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "collapsed_sections": [
        "2O-x5w1y9sQd",
        "zjWCVRdKNBj-",
        "rxAWWNuW51bV",
        "dm6ACAT2P7iT",
        "hb9ack3eR9OL"
      ],
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
